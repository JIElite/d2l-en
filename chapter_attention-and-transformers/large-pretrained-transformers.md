# Large Pretrained Transformers
:label:`sec_large-pretrained-transformers`


## Encoder-Only

### BERT

<!--
BERT :cite:`Devlin.Chang.Lee.ea.2018`
-->

BERT variants:

<!--
XLNET :cite:`yang2019xlnet`
RoBERTa :cite:`liu2019roberta`
ALBERT :cite:`lan2019albert`
DistilBERT :cite:`sanh2019distilbert`
ELECTRA :cite:`clark2019electra`
-->

### Swin Transformer

<!--
Swin Transformer :cite:`liu2021swin`
-->


## Encoder-Decoder


### T5

<!--
BART :cite:`lewis2019bart`
T5 :cite:`raffel2020exploring`
-->

### MAE

<!--
MAE :cite:`he2022masked`
-->


## Decoder-Only 


### GPT

<!--
GPT-1 :cite:`Radford.Narasimhan.Salimans.ea.2018`
GPT-2 :cite:`Radford.Wu.Child.ea.2019`
GPT-3 :cite:`brown2020language`
-->

### iGPT

<!--
iGPT :cite:`chen2020generative`
-->

## Scaling Up

### Scaling Laws

<!--
Scaling laws for neural LM :cite:`kaplan2020scaling`
Scaling laws for transfer :cite:`hernandez2021scaling`
Scale efficiently :cite:`tay2021scale`
-->


### Larger Models and Emergent Abilities


<!--
GLaM :cite:`du2021glam`
Gopher :cite:`rae2021scaling`
Megatron-Turing NLG 530B :cite:`smith2022using`

LaMDA :cite:`thoppilan2022lamda`
Chinchilla :cite:`hoffmann2022training`
Gopher :cite:`zhang2022opt`
PaLM :cite:`chowdhery2022palm`
-->


<!--
Emergent Abilities :cite:`wei2022emergent`
-->


### More Modalities

<!--
CLIP :cite:`radford2021learning`
DALL-E :cite:`ramesh2021zero`
DALL-E-2 :cite:`ramesh2022hierarchical`
Flamingo :cite:`alayrac2022flamingo`
Imagen :cite:`saharia2022photorealistic`
Generalist Agent :cite:`reed2022generalist`
-->

## Summary



## Exercises


[Discussions](https://discuss.d2l.ai/t/)



## References


